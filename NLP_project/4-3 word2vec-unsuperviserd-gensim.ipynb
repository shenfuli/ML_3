{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在gensim 中已经基于fastText 即成来c++ 和 python 接口的版本，我们可以直接使用gensim fasttext <br>\n",
    "\n",
    "目的： 通过fasttext库 需要学习的内容 <br>\n",
    "1. training word－embedding models  <br>\n",
    "2. saving & loading models \n",
    "3. performing simililary operations & vector  <br>\n",
    "4. fasttext vs word2vec  <br>\n",
    "\n",
    "\n",
    "[1]gensim 官方网站 <br>\n",
    "https://radimrehurek.com/gensim/models/fasttext.html  <br>\n",
    "[2]fastText word vector-Enriching word vector with Subword information <br>\n",
    "https://arxiv.org/abs/1607.04606 <br>\n",
    "[3]fastText by gensim version . notebook tutorial <br>\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb <br>\n",
    "[4]gensim-word2vec <br>\n",
    "http://www.52nlp.cn/tag/gensim-word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents size = 125836\n",
      "writing data to fasttext unsupervised learning format...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "#停用词\n",
    "stopwords_data_path = \"data/stopwords.txt\"\n",
    "stopwords_data_df = pd.read_csv(stopwords_data_path,encoding=\"utf-8\",sep=\"\\t\",index_col=None,quoting=3,names=[\"stopword\"])\n",
    "\n",
    "# 准备数据\n",
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "stopwords = stopwords_data_df.stopword.values.tolist()\n",
    "technology = df_technology.content.apply(lambda line: line.strip()).values.tolist()\n",
    "car = df_car.content.apply(lambda line: line.strip()).values.tolist()\n",
    "entertainment = df_entertainment.content.apply(lambda line: line.strip()).values.tolist()\n",
    "military = df_military.content.apply(lambda line: line.strip()).values.tolist()\n",
    "sports = df_sports.content.apply(lambda line: line.strip()).values.tolist()\n",
    "\n",
    "# 分词和中文处理\n",
    "'''\n",
    "5类数据最终处理的格式：\n",
    "word1 word2 word3 technology \n",
    "word1 word2 word3 word4 technology \n",
    "word1 word2 word4 car \n",
    "'''\n",
    "\n",
    "'''\n",
    "    lines: 文章的一条记录\n",
    "    sentences: 返回的数据列表\n",
    "    category： 文章的类别\n",
    "'''\n",
    "def preprocess_text(lines, documents):\n",
    "    for line in lines:\n",
    "        segs = jieba.lcut(line)\n",
    "        segs = [seg for seg in segs if len(seg) > 1 and seg not in stopwords]\n",
    "        data = \" \".join(segs)\n",
    "        documents.append(data)\n",
    "\n",
    "# 生成fasttext 无监督学习的样本数据\n",
    "documents = []\n",
    "preprocess_text(technology, documents)\n",
    "preprocess_text(entertainment, documents)\n",
    "preprocess_text(car, documents)\n",
    "preprocess_text(military, documents)\n",
    "preprocess_text(sports, documents)\n",
    "\n",
    "# ## 样本顺序打乱 并打印训练格式的样本\n",
    "import random\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(\"documents size = {0}\".format(len(documents)))\n",
    "print(\"writing data to fasttext unsupervised learning format...\")\n",
    "data_path = \"data_sample/fasttext_unsupervised_train_data.txt\"\n",
    "## TODO 存储文件需要utf-8\n",
    "with open(data_path,\"w\") as f_write:\n",
    "    for document in documents:\n",
    "        line = document.strip()\n",
    "        f_write.write(line + \"\\n\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load total sample size = 125838\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import multiprocessing\n",
    "from time import time\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "sentences=[]\n",
    "'''\n",
    "    >>> from gensim.models import Word2Vec\n",
    "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "    >>> model = Word2Vec(sentences, min_count=1)\n",
    "'''\n",
    "data_path = \"data_sample/fasttext_unsupervised_train_data.txt\"\n",
    "with open(data_path,\"r\",encoding='UTF-8') as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"load total sample size = {0}\".format(len(lines)))\n",
    "    for line in lines:\n",
    "        line_lst = line.strip().split(\" \")\n",
    "        sentences.append(line_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['中新网',\n",
       "  '长兴',\n",
       "  '日电',\n",
       "  '王逸飞',\n",
       "  '这是',\n",
       "  '美妙',\n",
       "  '体验',\n",
       "  '中国',\n",
       "  '参加',\n",
       "  '比赛',\n",
       "  '一种',\n",
       "  '中国',\n",
       "  '风景',\n",
       "  '第一个',\n",
       "  '太湖',\n",
       "  '图影',\n",
       "  '国际',\n",
       "  '半程',\n",
       "  '马拉松赛',\n",
       "  '终点',\n",
       "  '肯尼亚',\n",
       "  '运动员',\n",
       "  '拉蒙',\n",
       "  '享受',\n",
       "  '比赛'],\n",
       " ['疯岳',\n",
       "  '佳人',\n",
       "  '岳云鹏',\n",
       "  '痴情',\n",
       "  '护工',\n",
       "  '身份',\n",
       "  '惊喜',\n",
       "  '回归',\n",
       "  '偶遇',\n",
       "  '少时',\n",
       "  '女神',\n",
       "  '袁姗姗',\n",
       "  '萌生',\n",
       "  '追爱',\n",
       "  '愿望',\n",
       "  '背后',\n",
       "  '爱情',\n",
       "  '操作',\n",
       "  '助攻',\n",
       "  '更是',\n",
       "  '信心',\n",
       "  '大涨',\n",
       "  '誓要',\n",
       "  '虏获',\n",
       "  '女神',\n",
       "  '芳心',\n",
       "  '一同',\n",
       "  '曝光',\n",
       "  '剧照',\n",
       "  '两人',\n",
       "  '甜蜜',\n",
       "  '虐狗',\n",
       "  '细节',\n",
       "  '展露',\n",
       "  '温馨',\n",
       "  '进餐',\n",
       "  '把酒言欢',\n",
       "  '卖萌',\n",
       "  '耍宝',\n",
       "  '搞怪',\n",
       "  '自拍']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 14:34:53,360 : INFO : collecting all words and their counts\n",
      "2019-03-05 14:34:53,361 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-05 14:34:53,463 : INFO : PROGRESS: at sentence #10000, processed 279869 words, keeping 48298 word types\n",
      "2019-03-05 14:34:53,536 : INFO : PROGRESS: at sentence #20000, processed 559538 words, keeping 70601 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec model start....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 14:34:53,624 : INFO : PROGRESS: at sentence #30000, processed 838285 words, keeping 86966 word types\n",
      "2019-03-05 14:34:53,710 : INFO : PROGRESS: at sentence #40000, processed 1116414 words, keeping 100453 word types\n",
      "2019-03-05 14:34:53,787 : INFO : PROGRESS: at sentence #50000, processed 1394624 words, keeping 111713 word types\n",
      "2019-03-05 14:34:53,865 : INFO : PROGRESS: at sentence #60000, processed 1667450 words, keeping 121701 word types\n",
      "2019-03-05 14:34:53,950 : INFO : PROGRESS: at sentence #70000, processed 1944698 words, keeping 130560 word types\n",
      "2019-03-05 14:34:54,033 : INFO : PROGRESS: at sentence #80000, processed 2223065 words, keeping 138782 word types\n",
      "2019-03-05 14:34:54,114 : INFO : PROGRESS: at sentence #90000, processed 2499529 words, keeping 146573 word types\n",
      "2019-03-05 14:34:54,197 : INFO : PROGRESS: at sentence #100000, processed 2775387 words, keeping 154030 word types\n",
      "2019-03-05 14:34:54,279 : INFO : PROGRESS: at sentence #110000, processed 3050148 words, keeping 160706 word types\n",
      "2019-03-05 14:34:54,361 : INFO : PROGRESS: at sentence #120000, processed 3326561 words, keeping 166859 word types\n",
      "2019-03-05 14:34:54,412 : INFO : collected 170432 word types from a corpus of 3487966 raw words and 125838 sentences\n",
      "2019-03-05 14:34:54,415 : INFO : Loading a fresh vocabulary\n",
      "2019-03-05 14:34:54,570 : INFO : effective_min_count=5 retains 52514 unique words (30% of original 170432, drops 117918)\n",
      "2019-03-05 14:34:54,572 : INFO : effective_min_count=5 leaves 3294866 word corpus (94% of original 3487966, drops 193100)\n",
      "2019-03-05 14:34:54,766 : INFO : deleting the raw counts dictionary of 170432 items\n",
      "2019-03-05 14:34:54,769 : INFO : sample=0.001 downsamples 4 most-common words\n",
      "2019-03-05 14:34:54,771 : INFO : downsampling leaves estimated 3278580 word corpus (99.5% of prior 3294866)\n",
      "2019-03-05 14:34:54,968 : INFO : estimated required memory for 52514 words and 100 dimensions: 68268200 bytes\n",
      "2019-03-05 14:34:54,968 : INFO : resetting layer weights\n",
      "2019-03-05 14:34:55,647 : INFO : training model with 4 workers on 52514 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-05 14:34:56,662 : INFO : EPOCH 1 - PROGRESS: at 20.75% examples, 680505 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-05 14:34:57,672 : INFO : EPOCH 1 - PROGRESS: at 45.88% examples, 748815 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-05 14:34:58,681 : INFO : EPOCH 1 - PROGRESS: at 74.58% examples, 809084 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:34:59,608 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 14:34:59,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 14:34:59,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 14:34:59,632 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 14:34:59,636 : INFO : EPOCH - 1 : training on 3487966 raw words (3278543 effective words) took 4.0s, 823632 effective words/s\n",
      "2019-03-05 14:35:00,685 : INFO : EPOCH 2 - PROGRESS: at 20.16% examples, 644296 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-05 14:35:01,697 : INFO : EPOCH 2 - PROGRESS: at 42.70% examples, 687565 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:02,697 : INFO : EPOCH 2 - PROGRESS: at 68.21% examples, 735957 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:03,699 : INFO : EPOCH 2 - PROGRESS: at 92.38% examples, 748459 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:03,985 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 14:35:03,999 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 14:35:04,003 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 14:35:04,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 14:35:04,009 : INFO : EPOCH - 2 : training on 3487966 raw words (3278510 effective words) took 4.4s, 752288 effective words/s\n",
      "2019-03-05 14:35:05,034 : INFO : EPOCH 3 - PROGRESS: at 23.84% examples, 777410 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-05 14:35:06,044 : INFO : EPOCH 3 - PROGRESS: at 47.36% examples, 769281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-05 14:35:07,046 : INFO : EPOCH 3 - PROGRESS: at 72.52% examples, 787413 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:08,052 : INFO : EPOCH 3 - PROGRESS: at 96.73% examples, 786350 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:08,155 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 14:35:08,166 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 14:35:08,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 14:35:08,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 14:35:08,183 : INFO : EPOCH - 3 : training on 3487966 raw words (3278547 effective words) took 4.2s, 787498 effective words/s\n",
      "2019-03-05 14:35:09,211 : INFO : EPOCH 4 - PROGRESS: at 24.37% examples, 790578 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-05 14:35:10,226 : INFO : EPOCH 4 - PROGRESS: at 49.37% examples, 797124 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-05 14:35:11,233 : INFO : EPOCH 4 - PROGRESS: at 73.68% examples, 795175 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-05 14:35:12,094 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 14:35:12,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 14:35:12,106 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 14:35:12,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 14:35:12,118 : INFO : EPOCH - 4 : training on 3487966 raw words (3278632 effective words) took 3.9s, 834696 effective words/s\n",
      "2019-03-05 14:35:13,142 : INFO : EPOCH 5 - PROGRESS: at 28.37% examples, 925794 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-05 14:35:14,157 : INFO : EPOCH 5 - PROGRESS: at 56.79% examples, 920431 words/s, in_qsize 5, out_qsize 2\n",
      "2019-03-05 14:35:15,159 : INFO : EPOCH 5 - PROGRESS: at 85.81% examples, 928593 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-05 14:35:15,684 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-05 14:35:15,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-05 14:35:15,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-05 14:35:15,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-05 14:35:15,712 : INFO : EPOCH - 5 : training on 3487966 raw words (3278495 effective words) took 3.6s, 914777 effective words/s\n",
      "2019-03-05 14:35:15,713 : INFO : training on a 17439830 raw words (16392727 effective words) took 20.1s, 816998 effective words/s\n",
      "2019-03-05 14:35:15,736 : INFO : saving Word2Vec object under model/word2vec_gensim, separately None\n",
      "2019-03-05 14:35:15,738 : INFO : not storing attribute vectors_norm\n",
      "2019-03-05 14:35:15,739 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec model end....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 14:35:16,735 : INFO : saved model/word2vec_gensim\n",
      "2019-03-05 14:35:16,736 : INFO : storing vocabulary in model/vocabulary\n",
      "2019-03-05 14:35:16,969 : INFO : storing 52514x100 projection weights into model/word2vec_org\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total procesing time: 27 seconds\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "begin = time()\n",
    "print(\"training word2vec model start....\")\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(\"training word2vec model end....\")\n",
    "model.save(\"model/word2vec_gensim\")\n",
    "model.wv.save_word2vec_format(\"model/word2vec_org\", \"model/vocabulary\", binary=False)\n",
    "end = time()\n",
    "print(\"Total procesing time: %d seconds\" % (end - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型并预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 14:35:38,049 : INFO : loading Word2Vec object from model/word2vec_gensim\n",
      "2019-03-05 14:35:38,505 : INFO : loading wv recursively from model/word2vec_gensim.wv.* with mmap=None\n",
      "2019-03-05 14:35:38,507 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-05 14:35:38,508 : INFO : loading vocabulary recursively from model/word2vec_gensim.vocabulary.* with mmap=None\n",
      "2019-03-05 14:35:38,509 : INFO : loading trainables recursively from model/word2vec_gensim.trainables.* with mmap=None\n",
      "2019-03-05 14:35:38,511 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-05 14:35:38,513 : INFO : loaded model/word2vec_gensim\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"model/word2vec_gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算相关词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-05 14:35:39,601 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('亚洲', 0.6185275316238403)\n",
      "('全世界', 0.5958456993103027)\n",
      "('我国', 0.5894688963890076)\n",
      "('全球', 0.5791220664978027)\n",
      "('欧洲', 0.5742227435112)\n",
      "('海外', 0.5666359663009644)\n",
      "('GTI', 0.560455858707428)\n",
      "('日本', 0.5581508874893188)\n",
      "('中外', 0.5518802404403687)\n",
      "('东南亚', 0.5481557846069336)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\opt\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word_similar = model.wv.most_similar(\"中国\",topn=10)\n",
    "for wv in word_similar:\n",
    "    print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('社团', 0.7288618087768555)\n",
      "('篮球', 0.7183200120925903)\n",
      "('OCA', 0.7174619436264038)\n",
      "('体育产业', 0.7131071090698242)\n",
      "('智美', 0.7096304893493652)\n",
      "('体育赛事', 0.6921861171722412)\n",
      "('幼儿', 0.6896519660949707)\n",
      "('ELITE12', 0.6831167340278625)\n",
      "('篮球赛', 0.6772657632827759)\n",
      "('电子竞技', 0.673592209815979)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\opt\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word_similar = model.wv.most_similar(\"体育\",topn=10)\n",
    "for wv in word_similar:\n",
    "    print(wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1522624 , -0.13746184,  1.1977522 , -1.7664257 , -1.2219906 ,\n",
       "       -0.13999386,  3.8569906 ,  1.9267634 , -0.2056719 ,  1.4515932 ,\n",
       "       -0.26038316,  0.7853203 ,  1.5362432 ,  1.612691  , -0.49071452,\n",
       "        0.09172805,  1.0166246 , -0.7725363 , -2.1228657 ,  1.8180988 ,\n",
       "       -0.7423364 ,  1.2573221 , -2.612258  , -0.1093672 , -0.05340657,\n",
       "        0.4440791 , -2.867494  ,  2.4035149 ,  1.477893  ,  1.1256294 ,\n",
       "        0.1267563 , -3.2468925 ,  0.02933542,  0.21708433, -0.6532297 ,\n",
       "        1.1725346 , -0.45818368,  1.1296314 , -2.5563946 ,  1.0510018 ,\n",
       "       -0.4650817 , -0.70601475,  1.609785  , -2.0332994 , -0.7391503 ,\n",
       "        0.9180342 , -0.10604601, -0.44434017,  1.4818689 ,  1.1785319 ,\n",
       "       -0.8409995 ,  1.7344096 ,  0.25315493, -0.7753752 , -0.4193366 ,\n",
       "        1.5070014 ,  0.7722693 , -1.2543797 ,  0.13457158, -0.33472705,\n",
       "        0.3681033 ,  1.5747145 , -1.0378097 , -1.2138793 , -1.4531838 ,\n",
       "        2.1527858 , -0.57833695,  0.03880399,  0.59410256,  0.83228964,\n",
       "       -1.0635226 , -0.28519487,  2.0247905 , -1.9030074 , -1.2519052 ,\n",
       "        1.7847701 ,  1.7570438 ,  0.14460346,  2.856654  ,  1.1490136 ,\n",
       "       -1.9783248 ,  0.63963944, -0.6717114 ,  0.6543152 ,  0.09860644,\n",
       "        0.97218883,  0.2541427 , -1.8268261 , -2.2858498 , -3.0723805 ,\n",
       "        0.50224936,  1.7063022 ,  0.9277694 , -2.597827  ,  1.0966156 ,\n",
       "        2.4353929 , -0.13591428, -0.70739686, -0.59310853, -0.23997895],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector(\"中国\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于句子相似度，我们可以通过jieba分词，然后计算平均的每个词的向量，然后通过cos计算相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算句子相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每个句子的所有词向量取均值，来生成一个句子的vector\n",
    "def build_sentence_vector(text, size, imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\opt\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "dim=100\n",
    "text1=\"金牌 班底 保证 品质\"\n",
    "text2=\"同质化 谈话 节目 落后 生产力\"\n",
    "text_1_vector=build_sentence_vector(text1.split(),dim,model)\n",
    "text_2_vector=build_sentence_vector(text2.split(),dim,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46178815,  0.54263485,  0.342274  ,  0.33006503,  0.57206069,\n",
       "        -0.16667145,  0.26062876, -0.03352004, -0.49800857, -0.27923173,\n",
       "        -0.54460798,  0.45641993, -0.86080161,  0.27758218, -1.20197947,\n",
       "        -0.85763153,  0.18923374,  0.56686543, -0.13618138, -0.55435745,\n",
       "        -0.04517515, -0.00288211, -0.51541702, -0.30828042,  0.38201458,\n",
       "        -0.35516161, -0.63815296, -1.00554959,  0.2028668 ,  0.30985681,\n",
       "        -0.66128606, -1.29287154,  0.71256616, -0.21823826, -1.27554249,\n",
       "         0.07252802, -0.77478203,  0.39762783,  0.04207839,  0.23404703,\n",
       "         0.0095697 ,  0.44164924,  0.59939779, -0.6150204 ,  0.71697787,\n",
       "        -0.15711732,  0.75601466, -0.36745521,  0.38619499,  0.35793606,\n",
       "         1.27060813,  1.03245136, -0.66065533,  0.24219526, -0.05416447,\n",
       "        -0.2166793 ,  0.29253173,  0.5787945 ,  0.35621924,  0.63288261,\n",
       "         0.85758192, -0.03926025,  0.02800246, -0.26811657, -0.29975752,\n",
       "         0.72933676,  0.8842163 , -0.05754145,  0.68540771, -0.75837145,\n",
       "        -0.13732955, -0.29157856, -0.62902301, -0.67138755, -0.38588687,\n",
       "         0.72653084,  0.29640979,  0.38456057, -0.62274403,  0.29412833,\n",
       "        -0.47906005,  0.30701131, -0.32883508,  0.93239076, -0.58971471,\n",
       "         0.5548587 ,  0.50708896,  0.60791604, -0.07052731, -0.03858088,\n",
       "         0.62220557,  0.09436131, -0.22253174, -0.28135327, -0.2386776 ,\n",
       "         0.79282164, -1.19032081,  0.21053455, -1.10311422, -0.63118178]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19758285,  0.14212382,  0.25690537, -0.29763222,  0.60107128,\n",
       "         0.01719552, -0.11725751,  0.09392206, -0.44982149, -0.07735251,\n",
       "        -0.59122791,  0.02265279, -0.22360392,  0.49646062, -0.52764289,\n",
       "        -0.61612995,  0.56112498, -0.49715714, -0.26017753, -0.82724852,\n",
       "        -0.22608847,  0.78778634,  0.32907611,  0.14067345, -0.21323277,\n",
       "        -0.26892053,  0.22976173, -0.0966771 ,  0.05375936,  0.75196879,\n",
       "        -0.74821596, -0.47816626,  0.66834318, -0.09158119, -0.84514935,\n",
       "        -0.20762842,  0.20921763,  0.22566281, -1.14158846,  0.38871826,\n",
       "        -0.30845115,  0.30048392,  0.88220437,  0.25662818,  0.52279129,\n",
       "         0.00645755,  0.41170232,  0.50595607, -0.23230699, -0.28025027,\n",
       "         0.65846098,  0.38681295, -0.68398157,  0.00134029,  0.17166461,\n",
       "         0.73254093,  0.05251779,  0.04331314, -0.10223302,  0.08562548,\n",
       "        -0.15791062,  0.0161509 ,  0.32867494,  0.13696693,  0.28452277,\n",
       "        -0.02343155,  0.19672216, -0.29948106,  0.22144295,  0.27680188,\n",
       "        -0.07186693, -0.12993715, -0.59701481, -0.93063201,  0.49879758,\n",
       "         0.43081313, -0.18977145,  0.15174447, -0.97118607,  0.04525213,\n",
       "         0.27209772, -0.02397978, -0.23861387,  0.80554246,  0.39541133,\n",
       "        -0.52649213,  0.77994532,  0.14057144,  0.20748066, -0.60512583,\n",
       "         0.27666825, -0.4297963 ,  0.24022749, -0.71247969,  0.29439214,\n",
       "        -0.07900274,  0.2090569 ,  0.20950014, -0.882118  , -0.12189941]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的过程就不描述了，我们通过cos公式就可以计算出来了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结，word2vec 和 fasttext 区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/cdyx369/article/details/80579700 <br>\n",
    "https://www.jianshu.com/p/da0a5edeca3d<br>\n",
    "https://www.sohu.com/a/198733424_642762<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText 原理\n",
    "语言来说语序是很重要的，‘我爱你’的词袋特征<br>\n",
    "\n",
    "在CBOW窗口内加入ngram特征（比如2gram）就会得到额外的信息，第一个句子的2gram特征是‘我爱’和‘爱你’，第二个句子的特征是‘你爱’ 和‘爱我’，这样就把上下文完全相同的两个句子区分开了，这种做法就是fastText的做法<br>\n",
    "\n",
    "fastText简单来说就是将句子中的每个词先通过一个lookup层映射成词向量，然后对词向量取平均作为真个句子的句子向量，然后直接用线性分类器进行分类，从而实现文本分类.<br>\n",
    "\n",
    "fastText完全是线性的，没有非线性隐藏层，得到的结果和有非线性层的网络差不多.对于句子结构简单的文本来说，但是这种方法显然没有考虑词序信息.fastText就不如有隐藏层等非线性结构的网络效果好\n",
    "‘The movie is not very good , but i still like it . ’  <br>\n",
    "‘The movie is very good , but i still do not like it .’ <br>\n",
    "‘I do not like it , but the movie is still very good .’ <br>\n",
    "这几个句子的词序差不多，用到的词也差不多，但是表达的意思是完全相反的，如果直接把词向量取平均，显然得到的平均词向量也是相差不到，在经过线性分类器分类很容易把这两个不同的类别分到同一类里，所以fastText很难学出词序对句子语义的影响，对复杂任务还是需要用复杂网络学习任务的语义表达。 \n",
    "\n",
    "\n",
    "\n",
    "对于长文本的文本分类任务来说，就算是用词袋模型，效果也不差<br>\n",
    "在Word2vec的基础上，把N-grams也当做词来训练word2vec，最后每个词的向量将由这个Ngrams得出。这个改进能提升模型对morphology的效果, 即”字面上”相似的词语distance也会小一些.<br>\n",
    "\n",
    "(1)fastText包含三部分：模型结构，层次softmax，ngram  <br>\n",
    "(2)fastText和Word2vector的CBOW模型很相似，不同的是fastText预测句子标签，CBOW预测中间词 <br>\n",
    "(3)fastText的Ngram特征  <br>\n",
    "词向量训练中常用的特征是词袋模型，但是词袋模型不能引入词序信息，比如‘我爱你’的词袋特征是‘我’ ‘爱’ ‘你’ ，‘你爱我’的词袋特征是‘你’ ‘爱’ ‘我’，这两个句子的特征是完全相同的。如果加入Ngram，这两个句子的特征就不同了，‘我爱你’的特征由加入了‘我爱’和‘爱你’，你爱我的特征有加入了‘你爱’和‘爱我’，用Ngram得到的特征是完全不同的，这两句话就能区别开了,当然，‘你爱’和‘爱我’这两个词也要包含在词典里，所以会有词频较少的词要舍弃，否则计算量太大的问题。 <br>\n",
    "\n",
    "\n",
    "隐藏表征<br>\n",
    "在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。这类表征被称为词袋（bag of words）（此处忽视词序）。在 fastText中也使用向量表征单词 n-gram来将局部词序考虑在内，这对很多文本分类问题来说十分重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层次softmax（hierarchical Softmax） \n",
    "层次softmax可以提高训练速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText与CBOW的区别 \n",
    "fastText的模型和CBOW的模型结构一样，虽然结构一样，但是仍有不同 <br>\n",
    "一、目的不一样，fastText是用来做文本分类的，虽然中间也会产生词向量，但词向量是一个副产物，而CBOW就是专门用来训练词向量的工具。 <br>\n",
    "fastText的输出层是预测句子的类别标签，而CBOW的输出层是预测中间词； <br>\n",
    "fastText的输入层是一个句子的每个词以及句子的ngram特征<br>\n",
    "fastText是一个文本分类算法，是一个有监督模型，有额外标注的标签 。<br>\n",
    "CBOW是一个训练词向量的算法，是一个无监督模型，没有额外的标签，其标准是语料本身，无需额外标注。<br>\n",
    "\n",
    "fastText做文本分类的关键点是极大地提高了训练速度（在要分类的文本类别很多的情况下，比如500类），原因是在输出层采用了层级softmax，层级softmax如何提高训练速度在上面CBOW的层级softmax中已经介绍了，在这里就是将叶节点有词频变成文本分类数据集中每种类别的样本数量，霍夫曼树的结构也可以处理类别不均衡的问题（每种类别的样本数目不同），频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高（意思是数目多的样本深度小，那很多样本都只需乘一次就把概率计算出来了，自然就快）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-model \n",
    "skip-gram模型是已知中间词，预测他的上下文\n",
    "为提高训练速度，输出层仍是采用层次softmax，构造霍夫曼树来做，除了构造霍夫曼树，word2vec还提出了一种提高训练速度的方法，叫做负采样。\n",
    "\n",
    "### 负采样（Negative Sampling） \n",
    "目的是提高训练速度并改善所得词向量的质量，以CBOW中的负采样为例，目标词w是正样本，其他的词就是负样本了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec \n",
    "word2vec是Google与2013年开源推出的一个用于获取word vecter的工具包，利用神经网络为单词寻找一个连续向量表示。\n",
    "\n",
    "word2vec有两种网络模型，分别为：\n",
    "* Continous Bag of Words Model （CBOW）- 根据上下文，预测中间词语\n",
    "* Skip-Gram-根据中间词，预测上下文\n",
    "\n",
    "\n",
    "## FastText词向量与word2vec对比\n",
    "\n",
    "FastText= word2vec中 cbow + h-softmax的灵活使用\n",
    "\n",
    "灵活体现在两个方面： \n",
    "1. 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； \n",
    "2. 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容；\n",
    "\n",
    "两者本质的不同，体现在 h-softmax的使用。 \n",
    "Wordvec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的 h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。 \n",
    "\n",
    "fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
