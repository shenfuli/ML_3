{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用机器学习方法完成中文文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输入形态：\n",
    "\n",
    "CountVectorizer <br>\n",
    "bow : bag of words <br>\n",
    "ti-idf : text features <br>\n",
    "word2vec : cbow/skip-gram (单个维度没有物理含义) <br>\n",
    "* 传统机器学习模型：\n",
    "\n",
    "Naive Bayes <br>\n",
    "SVM <br>\n",
    "RF <br>\n",
    "GBDT\n",
    "* 深度学习模型：\n",
    "\n",
    "MLP <br>\n",
    "CNN <br>\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们试试用朴素贝叶斯完成一个中文文本分类器，一般在数据量足够，数据丰富度够的情况下，用朴素贝叶斯完成这个任务，准确度还是很不错的。\n",
    "\n",
    "机器学习的算法要取得好效果，离不开数据，咱们先把数据加载进来看看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好数据，我们挑选 科技、汽车、娱乐、军事、运动 总共5类文本数据进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "#停用词\n",
    "stopwords_data_path = \"data/stopwords.txt\"\n",
    "stopwords_data_df = pd.read_csv(stopwords_data_path,encoding=\"utf-8\",sep=\"\\t\",index_col=None,quoting=3,names=[\"stopword\"])\n",
    "\n",
    "# 准备数据\n",
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8', names=[\"id\", \"content\"], header=0)\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "stopwords = stopwords_data_df.stopword.values.tolist()\n",
    "technology = df_technology.content.apply(lambda line: line.strip()).values.tolist()\n",
    "car = df_car.content.apply(lambda line: line.strip()).values.tolist()\n",
    "entertainment = df_entertainment.content.apply(lambda line: line.strip()).values.tolist()\n",
    "military = df_military.content.apply(lambda line: line.strip()).values.tolist()\n",
    "sports = df_sports.content.apply(lambda line: line.strip()).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们抽取几天数据来看下数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = df_technology.content.apply(lambda line:line.strip()).values.tolist()\n",
    "car = df_car.content.apply(lambda line:line.strip()).values.tolist()\n",
    "entertainment = df_entertainment.content.apply(lambda line:line.strip()).values.tolist()\n",
    "military = df_military.content.apply(lambda line:line.strip()).values.tolist()\n",
    "sports = df_sports.content.apply(lambda line:line.strip()).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016年12月底，IDC发布调查报告显示，中国手机厂商于去年10月份在印度最主要的30个城市拿下了40%的市场份额。尽管三星仍然占据第一的位置，但二三名都被中国手机厂商拿下。其中，三星市场份额为26.1%，较上月增长15.8%，联想(包括摩托罗拉)以13.4%的份额位居第二，较上个月增长了50%，小米以10.7%的份额位列第三，较上月增长41.7%。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technology[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'从长远来看，当前我国新能源汽车市场还处于“政策驱动”的阶段，伴随着政府补贴政策进入退坡通道，行业或将进入发展的调整期，这将倒逼新能源汽车企业强化对核心技术的突破创新，推动车企通过提高自身综合实力来降本增效，大力提升新能源汽车的性价比，在退坡的周期内逐渐走上依托市场自行发展的道路。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'网络综艺与电视综艺在播出模式、观众群体以及节目板块等方面也都存在差异。在传播上，电视台比网络平台更容易产生爆款。尽管这几年电视综艺也在网站上线，但由于观众观看的时间相对一致，而网络节目的时间较随意，所以同一档节目在同一时间，电视聚集观众的能力要比互联网强。《爸爸去哪儿4》转成网络综艺播出时，虽然也因一些话题而引发讨论，但是因传统受众观看习惯的影响，而网络观看又具有随意性，缺乏即时的、一致的观看体验，而导致没有掀起前几季的全民讨论的热潮。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'火箭军某导弹旅参谋长 陈伟：'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "military[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“明年会继续争取吗？”媒体追问。此时的小德情绪显然有了波动，“到时候再说吧。我理解大家的工作，但也请你们理解我现在的处境。”'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports[900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词和中文处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5类数据最终处理的格式：<br>\n",
    "word1 word2 word3 technology <br>\n",
    "word1 word2 word3 word4 technology <br>\n",
    "word1 word2 word4 car <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5类数据最终处理的格式：\n",
    "word1 word2 word3 technology \n",
    "word1 word2 word3 word4 technology \n",
    "word1 word2 word4 car \n",
    "'''\n",
    "\n",
    "'''\n",
    "    lines: 文章的一条记录\n",
    "    sentences: 返回的数据列表\n",
    "    category： 文章的类别\n",
    "'''\n",
    "def preprocess_text(lines, file_name, category):\n",
    "                                                \n",
    "    with open(file_name,\"w\") as f:              \n",
    "        for line in lines:                      \n",
    "            segs = jieba.lcut(line)             \n",
    "            segs = [seg for seg in segs if len(s\n",
    "            data = \" \".join(segs)               \n",
    "            label = category                    \n",
    "            f.write(label + \"\\t\" + data + \"\\n\") \n",
    "\n",
    "## 处理 label data 格式的数据\n",
    "preprocess_text(technology, \"data_sample/technology_sample.txt\", \"technology\")\n",
    "preprocess_text(entertainment, \"data_sample/entertainment_sample.txt\", \"entertainment\")\n",
    "preprocess_text(car, \"data_sample/car_sample.txt\", \"car\")\n",
    "preprocess_text(military, \"data_sample/military_sample.txt\", \"military\")\n",
    "preprocess_text(sports, \"data_sample/sports_sample.txt\", \"sports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data_path = \"data_sample/data_label.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 合并文件\n",
    "category_lst = ['technology','entertainment','car','military','sports']\n",
    "with open(merge_data_path,\"w\") as f_write:\n",
    "    for category in category_lst:\n",
    "        file_path = \"data_sample/{0}_sample.txt\".format(category)\n",
    "        with open(file_path) as f:\n",
    "            for line in f.readlines():\n",
    "                f_write.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "count() takes exactly one argument (0 given)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d68c81a3db75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#documents.append((data[0],data[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: count() takes exactly one argument (0 given)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "## 打乱文件的顺序\n",
    "documents = []                     \n",
    "with open(merge_data_path,\"r\") as f:\n",
    "    for line in f.readlines()[0:10]:\n",
    "        data = line.split(\"\\t\")   \n",
    "        #documents.append((data[0],data[1]))\n",
    "        print(data[0],data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
