{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t.from_numpy(np.load(opt.embedding_path)['vector'])\n",
    "embedding_path = \"../word2vec/AJData.word2vec.model\"\n",
    "model = Word2Vec.load(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "False\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "# print(len(model.wv['好好']))\n",
    "# model.wv['法庭']\n",
    "a.append(model.wv['好好'])\n",
    "a.append(model.wv['法庭'])\n",
    "a = np.array(a)\n",
    "print(a.shape)\n",
    "print('法庭d' in model.wv)\n",
    "print(model.layer1_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_len = 0\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split('\\t')\n",
    "            one_data = line_list[1].split(' ')\n",
    "            tmp_len = len(one_data)\n",
    "            if tmp_len > max_sentence_len:\n",
    "                max_sentence_len = tmp_len\n",
    "            data.append(one_data)\n",
    "            labels.append(int(line_list[2]))\n",
    "        f.close()\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def show_text_len_distribution(data):\n",
    "    len_list = [len(text) for text in data]\n",
    "#     print(len_list[1:100])\n",
    "    step = 500\n",
    "    for k, g in groupby(sorted(len_list), key=lambda x: (x-1)//step):\n",
    "    #    dic['{}-{}'.format(k*step+1, (k+1)*step)] = len(list(g))\n",
    "        print('{}-{}'.format(k*step+1, (k+1)*step)+\":\"+str(len(list(g))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voabulary(data, vocabulary_size=50000):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, labels, dict_word2index, max_sentence_len=1000, label_size=8):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        new_labels.append(labels[i]-1) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line[:max_sentence_len])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int64), np.array(new_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, radio=0.7):\n",
    "    \"\"\"\n",
    "    将训练集分给为训练集和检验集\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * 0.7)\n",
    "    new_data1 = data[ : split_index]\n",
    "    new_data2 = data[split_index : ]\n",
    "    return new_data1, new_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class MingLueData(data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.len = X.shape[0]\n",
    "        self.x_data = X\n",
    "        self.y_data = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  20420\n",
      "1-500:6015\n",
      "501-1000:2406\n",
      "1001-1500:647\n",
      "1501-2000:303\n",
      "2001-2500:206\n",
      "2501-3000:125\n",
      "3001-3500:70\n",
      "3501-4000:55\n",
      "4001-4500:37\n",
      "4501-5000:33\n",
      "5001-5500:19\n",
      "5501-6000:17\n",
      "6001-6500:9\n",
      "6501-7000:7\n",
      "7001-7500:5\n",
      "7501-8000:6\n",
      "8001-8500:7\n",
      "8501-9000:4\n",
      "9001-9500:7\n",
      "9501-10000:5\n",
      "10501-11000:2\n",
      "11501-12000:1\n",
      "12001-12500:2\n",
      "12501-13000:3\n",
      "14501-15000:3\n",
      "15001-15500:1\n",
      "16001-16500:1\n",
      "17001-17500:1\n",
      "17501-18000:2\n",
      "20001-20500:1\n",
      "(7000, 1000)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/seg_sample_train.txt'\n",
    "data, labels = load_data(data_path)\n",
    "show_text_len_distribution(data)\n",
    "vocab_size = 100000\n",
    "max_text_len = 1000\n",
    "count, dict_word2index, dict_index2word = build_voabulary(data, vocabulary_size=vocab_size)\n",
    "train_data, train_labels = build_dataset(data, labels, dict_word2index, max_sentence_len=max_text_len)\n",
    "train_X, valid_X = split_data(train_data)\n",
    "train_y, valid_y = split_data(train_labels)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "dataset = MingLueData(train_X[:3000], train_y[:3000])\n",
    "train_loader = DataLoader(dataset=dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,\n",
    "                               num_workers=num_workers)\n",
    "dataset = MingLueData(valid_X[:1000], valid_y[:1000])\n",
    "valid_loader = DataLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN (\n",
      "  (embedding): Embedding(100000, 128)\n",
      "  (convs): ModuleList (\n",
      "    (0): Sequential (\n",
      "      (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): ReLU ()\n",
      "      (2): MaxPool1d (size=998, stride=998, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential (\n",
      "      (0): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
      "      (1): ReLU ()\n",
      "      (2): MaxPool1d (size=997, stride=997, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential (\n",
      "      (0): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
      "      (1): ReLU ()\n",
      "      (2): MaxPool1d (size=996, stride=996, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear (300 -> 8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "sentence_size = 1000\n",
    "feature_size = 100\n",
    "num_class = 8\n",
    "vocab_size = 100000\n",
    "window_sizes = [3, 4, 5]\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "                nn.Sequential(nn.Conv1d(in_channels=embedding_size, \n",
    "                                        out_channels=feature_size, \n",
    "                                        kernel_size=h),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool1d(kernel_size=sentence_size-h+1))\n",
    "                     for h in window_sizes\n",
    "                    ])\n",
    "        self.fc = nn.Linear(in_features=feature_size*len(window_sizes),out_features=num_class)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_x = self.embedding(x)\n",
    "        \n",
    "#         print(embed_x.size())\n",
    "#         # 4 x 1000 x 128  -> 4 x 128 x 1000\n",
    "        embed_x = embed_x.permute(0, 2, 1)\n",
    "#         print(embed_x.size())\n",
    "        out = [conv(embed_x) for conv in self.convs]\n",
    "#         for o in out:\n",
    "#             print(o.size())\n",
    "        out = torch.cat(out, dim=1)\n",
    "#         print(out.size(1))\n",
    "        out = out.view(-1, out.size(1))\n",
    "#         print(out.size())\n",
    "        out = F.dropout(input=out, p=dropout_rate)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "text_cnn = TextCNN()\n",
    "print(text_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "# weight_decay 等价与L2正则化\n",
    "optimizer = optim.Adam(params=text_cnn.parameters(),lr=learning_rate,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.056\n",
      "[1,   200] loss: 2.003\n",
      "[1,   300] loss: 2.086\n",
      "[1,   400] loss: 2.029\n",
      "[1,   500] loss: 2.032\n",
      "[1,   600] loss: 1.941\n",
      "[1,   700] loss: 1.955\n",
      "[2,   100] loss: 1.975\n",
      "[2,   200] loss: 1.943\n",
      "[2,   300] loss: 1.972\n",
      "[2,   400] loss: 1.940\n",
      "[2,   500] loss: 1.914\n",
      "[2,   600] loss: 1.878\n",
      "[2,   700] loss: 1.875\n",
      "[3,   100] loss: 1.935\n",
      "[3,   200] loss: 1.900\n",
      "[3,   300] loss: 1.939\n",
      "[3,   400] loss: 1.919\n",
      "[3,   500] loss: 1.901\n",
      "[3,   600] loss: 1.860\n",
      "[3,   700] loss: 1.862\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 3\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        texts, labels = data\n",
    "        \n",
    "        inputs, labels = Variable(texts), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = text_cnn(inputs)\n",
    "        loss = loss_fun(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def micro_avg_f1(predict_label, true_label, num_class):\n",
    "    N = len(predict_label)\n",
    "    m = num_class\n",
    "    w = Counter(true_label)\n",
    "    print(w)\n",
    "    score = 0\n",
    "    for i in range(m):\n",
    "        score += w[i] * f1(predict_label, true_label, i)\n",
    "\n",
    "    return score / float(N)\n",
    "\n",
    "\n",
    "def f1(predict_label, true_label, cur_label):\n",
    "    true_pos, false_pos = 0, 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(predict_label)):\n",
    "        if predict_label[i] == cur_label:\n",
    "            if true_label[i] == cur_label:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "        else:  # predict_label != cur_label\n",
    "            if true_label[i] == cur_label:\n",
    "                false_neg += 1\n",
    "    if true_pos == 0:\n",
    "        precision, recall = 0, 0\n",
    "    else:\n",
    "        precision = true_pos / float(true_pos + false_pos)\n",
    "        recall = true_pos / float(true_pos + false_neg)\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 2, 0, 6, 3, 6, 0, 0, 1]\n",
      "[6, 1, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Counter({6: 233, 1: 167, 5: 160, 0: 141, 4: 125, 2: 111, 3: 58, 7: 5})\n",
      "Micro-Averaged F1: 0.23868395004777757\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for data in valid_loader:\n",
    "    texts, labels = data\n",
    "    outputs = text_cnn(Variable(texts))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    true_labels.extend(labels)\n",
    "    predicted = [i[0] for i in predicted]\n",
    "    predicted_labels.extend(predicted)\n",
    "\n",
    "print(true_labels[:10])\n",
    "print(predicted_labels[:10])\n",
    "print(\"Micro-Averaged F1:\",micro_avg_f1(predicted_labels, true_labels, num_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
