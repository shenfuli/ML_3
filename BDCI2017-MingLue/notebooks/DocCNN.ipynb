{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    max_sentence_num = 500\n",
    "    max_sentence_len= 100\n",
    "    vocab_size = 100000\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, config):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_num = 0\n",
    "    max_sentence_len = 0\n",
    "    longest_sentence = \"\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split('\\t')\n",
    "            sentences = re.split(r'[。|；|！|？|，|（|）]', line_list[1])\n",
    "            tmp_num = len(sentences)\n",
    "            if tmp_num > max_sentence_num:\n",
    "                max_sentence_num = tmp_num\n",
    "            if tmp_num > config.max_sentence_num:\n",
    "                continue\n",
    "            one_data = []\n",
    "            for s in sentences:\n",
    "                words = s.split(' ')\n",
    "                tmp_len = len(words)\n",
    "                if tmp_len > max_sentence_len:\n",
    "                    max_sentence_len = tmp_len\n",
    "                    longest_sentence = words\n",
    "                one_data.append(words)\n",
    "            data.append(one_data[:config.max_sentence_len])\n",
    "            labels.append(int(line_list[2]))\n",
    "        f.close()\n",
    "    print(\"max sentence num:\", max_sentence_num)\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence num: 2250\n",
      "max sentence length:  257\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/seg_sample_train.txt'\n",
    "data, labels = load_data(data_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['公诉', '机关', '北京市', '昌平区', '人民检察院', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', ''], ['', '男', ''], ['', '27', '岁', ''], ['', '1986', '年', '11', '月', '10', '日', '出生', ''], ['', ''], ['', '因涉嫌', '犯', '盗窃罪', '于', '2014', '年', '6', '月', '20', '日', '被', '羁押', ''], ['', '同年', '7', '月', '3', '日', '被', '逮捕', ''], ['', '现', '羁押于', '北京市', '昌平区', '看守所', ''], ['', '北京市', '昌平区', '人民检察院', '以京昌检', '公诉', '刑诉', ''], ['', '2014', ''], ['', '610', '号', '起诉书', '指控', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '犯', '盗窃罪', ''], ['', '于', '2014', '年', '7', '月', '22', '日向', '本院', '提起公诉', ''], ['', '本院', '依法', '适用', '简易程序', ''], ['', '实行', '独任', '审判', ''], ['', '公开', '开庭', '进行', '了', '审理', ''], ['', '北京市', '昌平区', '人民检察院', '指派', '检察员', '夏', '文广', '出庭', '支持', '公诉', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '到庭', '参加', '诉讼', ''], ['', '本案', '现已', '审理', '终结', ''], ['', '北京市', '昌平区', '人民检察院', '起诉书', '指控', '：', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '于', '2014', '年', '6', '月', '20', '日', '16', '时', '50', '分许', ''], ['', '进入', '昌平区', '东小', '口镇', '兴旺', '地', '市场', '南门', '西侧', '赵', '&', 'times', ';', '&', 'times', ';', '烟', '酒店', '内', ''], ['', '趁', '四周', '无人', ''], ['', '将店', '内', '货架', '抽屉', '内', '的', '人民币', '5700', '余元', '及', '5000', '余元', '的', '手机', '充值卡', '窃走', ''], ['', '赵', '&', 'times', ';', '&', 'times', ';', '发现', '后', '随即', '追赶', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '在', '逃跑', '过程', '中', '分', '两次', '将', '所', '窃', '钱', '、', '卡', '抛撒', ''], ['', '后', '被', '抓获', ''], ['', '赵', '&', 'times', ';', '&', 'times', ';', '在', '追赶', '过程', '中', '捡拾', '被盗', '人民币', '1587', '元', ''], ['', '手机', '充值卡', '2960', '元', ''], ['', '上述事实', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '在', '庭审', '过程', '中未', '提出异议', ''], ['', '并', '有', '经过', '庭审', '质证', '、', '认证', '的', '被害人', '赵', '&', 'times', ';', '&', 'times', ';', '的', '陈述', ''], ['', '证人', '李', '&', 'times', ';', '&', 'times', ';', '、', '郭', '&', 'times', ';', '&', 'times', ';', '的', '证言', ''], ['', '辨认', '笔录', ''], ['', '公安机关', '出具', '的', '接', '报案', '和', '到案', '经过', ''], ['', '照片', ''], ['', '工作', '说明', ''], ['', '身份证明', '材料', ''], ['', '光盘', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '的', '供述', '等', '证据', '在案', '佐证', ''], ['', '足以认定', ''], ['']]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voabulary(data, vocab_size):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for doc in data:\n",
    "        for sentence in doc:\n",
    "            words.extend(sentence)\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, dict_word2index, dict_index2word = build_voabulary(data, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, labels, dict_word2index, config):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        new_labels.append(labels[i]-1) \n",
    "        new_doc = []\n",
    "        for sentence in data[i]:\n",
    "            new_sentence = []\n",
    "            for word in sentence:\n",
    "                if word in dict_word2index:\n",
    "                    index = dict_word2index[word]\n",
    "                else:\n",
    "                    index = 0    # UNK\n",
    "                new_sentence.append(index)\n",
    "            zero_num = config.max_sentence_len - len(new_sentence)\n",
    "            if zero_num > 0:\n",
    "                new_sentence.extend([0]*zero_num)\n",
    "            new_doc.append(new_sentence[:config.max_sentence_len])\n",
    "    \n",
    "        zero_num = config.max_sentence_num - len(new_doc)\n",
    "        while zero_num > 0:\n",
    "            new_doc.append([0]*config.max_sentence_len)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_doc[:config.max_sentence_num])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int64), np.array(new_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = build_dataset(data, labels, dict_word2index, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
