{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  20420\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_len = 0\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split('\\t')\n",
    "            one_data = line_list[1].split(' ')\n",
    "            tmp_len = len(one_data)\n",
    "            if tmp_len > max_sentence_len:\n",
    "                max_sentence_len = tmp_len\n",
    "            data.append(one_data)\n",
    "            labels.append(int(line_list[2]))\n",
    "        f.close()\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels\n",
    "\n",
    "data_path = '../data/seg_sample_train.txt'\n",
    "data, labels = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-500:6015\n",
      "501-1000:2406\n",
      "1001-1500:647\n",
      "1501-2000:303\n",
      "2001-2500:206\n",
      "2501-3000:125\n",
      "3001-3500:70\n",
      "3501-4000:55\n",
      "4001-4500:37\n",
      "4501-5000:33\n",
      "5001-5500:19\n",
      "5501-6000:17\n",
      "6001-6500:9\n",
      "6501-7000:7\n",
      "7001-7500:5\n",
      "7501-8000:6\n",
      "8001-8500:7\n",
      "8501-9000:4\n",
      "9001-9500:7\n",
      "9501-10000:5\n",
      "10501-11000:2\n",
      "11501-12000:1\n",
      "12001-12500:2\n",
      "12501-13000:3\n",
      "14501-15000:3\n",
      "15001-15500:1\n",
      "16001-16500:1\n",
      "17001-17500:1\n",
      "17501-18000:2\n",
      "20001-20500:1\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def show_text_len_distribution(data):\n",
    "    len_list = [len(text) for text in data]\n",
    "#     print(len_list[1:100])\n",
    "    step = 500\n",
    "    for k, g in groupby(sorted(len_list), key=lambda x: (x-1)//step):\n",
    "    #    dic['{}-{}'.format(k*step+1, (k+1)*step)] = len(list(g))\n",
    "        print('{}-{}'.format(k*step+1, (k+1)*step)+\":\"+str(len(list(g))))\n",
    "show_text_len_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voabulary(data, vocabulary_size=50000):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word\n",
    "\n",
    "count, dict_word2index, dict_index2word = build_voabulary(data, vocabulary_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 10 common words:  [['UNK', -1], ('，', 1035242), ('。', 478414), ('、', 428540), ('的', 356810), ('被告人', 279828), ('月', 222592), ('年', 206542), ('某某', 190734), ('日', 156288), ('在', 112568), ('2014', 111140), ('了', 104408), ('被', 102574), ('元', 100762), ('）', 84050), ('（', 84018), ('人民检察院', 72434), ('于', 72304), ('公诉', 67326)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most 10 common words: \", count[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, labels, dict_word2index, max_sentence_len=1000, label_size=8):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        one_label = [0] * label_size\n",
    "        one_label[labels[i]-1] = 1\n",
    "        new_labels.append(one_label) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line[:max_sentence_len])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int32), np.array(new_labels, dtype=np.int32)\n",
    "\n",
    "train_data, train_labels = build_dataset(data, labels, dict_word2index, max_sentence_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[4]))\n",
    "print(train_labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1000)\n",
      "(7000, 8)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, radio=0.7):\n",
    "    \"\"\"\n",
    "    将训练集分给为训练集和检验集\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * 0.7)\n",
    "    new_data1 = data[ : split_index]\n",
    "    new_data2 = data[split_index : ]\n",
    "    return new_data1, new_data2\n",
    "\n",
    "train_X, valid_X = split_data(train_data)\n",
    "train_y, valid_y = split_data(train_labels)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embedding_size = 300 # Dimension of the embedding vector.\n",
    "\n",
    "# num_sampled = 4 # Number of negative examples to sample.\n",
    "max_sentence_len = 1000\n",
    "vocabulary_size = 100000\n",
    "label_size = 8\n",
    "graph = tf.Graph()\n",
    "    \n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.int32, shape=[batch_size, max_sentence_len])\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=[batch_size,label_size])\n",
    "  tf_valid_dataset = tf.constant(valid_X, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([label_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([label_size]))\n",
    "  \n",
    "  def model(data): \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    # embedding_lookup()用法: http://blog.csdn.net/u013041398/article/details/60955847\n",
    "    embed = tf.nn.embedding_lookup(embeddings, data)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    sentence_embed = tf.reduce_mean(embed, axis=1)\n",
    "    \n",
    "    return tf.matmul(sentence_embed, tf.transpose(softmax_weights)) + softmax_biases\n",
    "\n",
    "\n",
    "  logits = model(tf_train_dataset)\n",
    "    \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.9, staircase=True)\n",
    "  optimizer = tf.contrib.layers.optimize_loss(loss, global_step=global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#   test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.14567 train accuracy: 0.0\n",
      "loss: 1.86938 train accuracy: 25.0\n",
      "loss: 1.4429 train accuracy: 56.25\n",
      "loss: 1.83364 train accuracy: 37.5\n",
      "loss: 1.55431 train accuracy: 37.5\n",
      "loss: 1.69195 train accuracy: 31.25\n",
      "loss: 1.68388 train accuracy: 37.5\n",
      "loss: 1.0841 train accuracy: 43.75\n",
      "loss: 1.307 train accuracy: 50.0\n",
      "loss: 0.893233 train accuracy: 56.25\n",
      "loss: 0.795051 train accuracy: 81.25\n",
      "loss: 0.812872 train accuracy: 75.0\n",
      "loss: 0.565707 train accuracy: 81.25\n",
      "loss: 0.473443 train accuracy: 87.5\n",
      "loss: 0.498563 train accuracy: 81.25\n",
      "loss: 0.424316 train accuracy: 87.5\n",
      "valid accuracy: 37.9333333333\n",
      "loss: 0.239726 train accuracy: 93.75\n",
      "valid accuracy: 39.2\n",
      "loss: 0.212022 train accuracy: 100.0\n",
      "valid accuracy: 37.9333333333\n",
      "loss: 0.266965 train accuracy: 93.75\n",
      "valid accuracy: 37.0333333333\n",
      "loss: 0.157774 train accuracy: 100.0\n",
      "valid accuracy: 38.2\n",
      "loss: 0.219489 train accuracy: 93.75\n",
      "valid accuracy: 37.6\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "    batch_data = train_X[offset:(offset + batch_size)]\n",
    "    batch_labels = train_y[offset:(offset + batch_size)]\n",
    "\n",
    "    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "    _, l,tp = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print(\"loss:\",l,\"acc:\",a,\"label:\",batch_labels,\"prediction:\",p)\n",
    "    if (step % 100 == 0):\n",
    "        train_acc = accuracy(tp, batch_labels)\n",
    "        print(\"loss:\", l, \"train accuracy:\", train_acc)\n",
    "        if step > num_steps * 0.7:\n",
    "            print( \"valid accuracy:\",accuracy(valid_prediction.eval(), valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经验总结(仅仅针对采样数据，由于量较少，可能不适用甚至相反)\n",
    "- 适当增加迭代次数可以提高准确度 （1001 -> 2001)\n",
    "- max_sentence_len: 1000 -> 2000 , acc: 40.3% -> 39.8%\n",
    "- embedding_size: 128 -> 300, acc: 40.3% -> 37%  特征表示变强，训练集准确度变高，但由于数据集过少，过拟合了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
